---
title: "Lerneinheit 7 und 8: Metadaten modellieren und Schnittstellen nutzen A (OpenRefine) (Teil 1/3 + 2/3)"
date: 2024-04-23
---

Dieser Blogeintrag umfasst die Inhalte aus den Vorlesungen vom 23.04.24 und 30.04.24 und behandelt das Thema Metadaten modellieren und Schnittstellen nutzen.

**OpenRefine**

Der Themenkomplex wurde am Beispiel von OpenRefine erläutert. OpenRefine ist ein in Java geschriebenes Programm mit einer grossen Community zur Bereinigung und Umwandlung von Daten. Das Programm liefert eine grafische Oberfläche, die einer klassischen Tabellenverarbeitungssoftware ähnelt und dient der Analyse, Bereinigung, Konvertierung und Anreicherung von Daten.  
 
Wir haben mit OpenRefine in der Lerneinheit 7 einige Übungen gemacht, wobei ich nachfolgend auf die spannendsten eingehe und jeweils die Funktionen **Facetten, Undo/Redo und Clustering** erkläre. 
 
- **Beispieldaten laden**

Zuerst haben wir ein Projekt erstellt und einen Datensatz aus dem Internet mit Zeitschriftenartikeln darin heruntergeladen. Das Programm generiert dann eine übersichtliche Tabelle  welche einen schnellen Überblick liefert:
 
![Alternativtext](https://jonasbracchi.github.io/bain-lerntagebuch/images/OpenRefine_Tabelle.png)
*OpenRefine Tabelle* 


- **Basisfunktionen (Facetten, Undo/Redo, Cluster)**

Nun ging es darum den Datensatz aufzuräumen. Wir haben uns dazu bei den **Facetten** die verschiedenen Sprachen anzeigen lassen. Dadurch wurde klar, dass unterschiedliche Abkürzungen existieren, welche wir vereinheitlicht haben (EN und English fusioniert).

![Alternativtext](https://jonasbracchi.github.io/bain-lerntagebuch/images/OpenRefine_Facetten.png)

*OpenRefine Facetten* 

OpenRefine ermöglicht das Faceting, was bedeutet, dass es die Daten analysiert und Kategorien basierend auf bestimmten Merkmalen erstellt. Diese Kategorien helfen dabei, Daten schnell zu durchsuchen und zu organisieren. 
 
Die Änderungen werden jeweils in der **Undo / Redo Historie** angezeigt und diesen als automatische Protokollführung. Diese unbegrenzte Rückgängig- und Wiederherstellungsfunktion ermöglicht den Benutzenden beliebig viele Schritte rückgängig zu machen oder wiederherstellen können. 
 
Mit der **Cluster Funktion** haben wir die Namen der Autoren vereinheitlicht, welche teilweise unterschiedlich geschrieben waren. Das Clustering ist eine Funktion, die ähnliche Datensätze automatisch gruppiert. OpenRefine verwendet Clustering-Algorithmen, um ähnliche Daten zu erkennen und sie zu gruppieren, was die Bereinigung grosser Datensätze erleichtert.

**Vorführung Reconciliation**

In der Lerneinheit 8 haben wir die Funktion Reconciliation von OpenRefine vorgeführt bekommen. Dazu haben wir in unserem Datensatz die Zeitschriftenartikel anhand der ISSN mit Informationen aus Wikidata ergänzt. Dazu haben wir Properties aus Wikidata ausgewählt, wie wir es auch im Modul "Grundlagen Semantischer Technologien" (GSET) gesehen haben.

**Weitere Übungen:**

In den letzten Übungen wurde es ziemlich technisch. Es ging darum, mittels OpenRefine eine CSV-Datei in das MARCXML-Format zu konvertieren, wobei die Templating Exporter-Funktion verwendet wurde. 

**Exkurs:**

Ergänzend zur BAIN Vorlesung möchte ich an dieser Stelle mit meinem Wissen aus GSET anknüpfen und vertieft auf diese Properties aus der Reconciliation Vorführung eingehen. In GSET haben wir Übungen gemacht, in welchen wir z. B. mit einer Query die durchschnittliche Anzahl Einwohner in der Schweiz berechnet haben. Dazu haben wir auf Wikidata.org jeweils Beispielorte gesucht und aus den Properties die Query hergeleitet:
![Alternativtext](https://jonasbracchi.github.io/bain-lerntagebuch/images/uebung_gset.png)

*GSET Übung* 
 
Diese Übungen stehen im Zusammenhang des semantischen Webs. Dabei geht es um die Idee, dass Maschinen im Web Bedeutung wahrnehmen und somit automatisch Zusammenhänge und Verknüpfungen finden können. Um dies möglich zu machen, haben wir ein Schichtenmodell aus verschiedenen Protokollen angesehen, auf welches ich gerne eingehen möchte:
![Alternativtext](https://jonasbracchi.github.io/bain-lerntagebuch/images/schichtenmodell.png)

*Schichtenmodell* 
 
Auf einem gemeinsamen Zeichensatz (URI, Unicode) basiert die Mark-Up-Language XML als standardisierte Sprache. Auf diese baut wiederum RDF (Resource Description Framework), ein Standard für den Datentransfer, auf. Es werden kleine zusammenhängende Datenkonstrukte in Form von Tripel mit der Struktur Ressource-Eigenschaft-Wert (bzw SPO - Subjekt, Prädikat, Objekt) ausgetauscht. Die Maschinen können diese dann weiterverarbeiten. RDF-S (RDF-Schema, eine semantische Erweiterung) ermöglicht es, diese Tripel zu sortieren und Gruppen zuzuweisen und somit einfache Ontologien (Vorstufen) zu bilden. Die Mitte bildet eine Ontologie, welche mit OWL (Ontology Web Language) geschrieben ist, mit SPARQL abgefragt werden kann und zusätzlich über ein Regelwerk (RIF: Rule Interchange Format) und eine formale Logik verfügt. Die Schichten Proof und Trust dienen der Überprüfung der Wahrheitsfrage. Die oberste Schicht, die Anwenderschicht kümmert sich um die Präsentation/Ausgabe für und die Interaktion mit den Benutzenden. Über die unteren Schichten hinweg zieht sich die Kryptographie, die Verschlüsselung der Daten.
 

