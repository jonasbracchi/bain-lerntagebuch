---
title: "Lerneinheit 10: Suchmaschinen und Discovery-Systeme (Teil 1/2)"
date: 2024-05-21
---

Zu Beginn der Vorlesung am 21.05.2024 gab es einen Nachtrag zur letzten Vorlesung "Metadaten modellieren und Schnittstellen nutzen". Dazu haben wir weitere Tools zur Metadatentransformation angeschaut: Catmandu (Perl), Metafacture (Java) und MarcEdit (MARC21). Wichtig dabei zu erwähnen ist, dass die Wahl der passenden Software  sich am spezifischen Anwendungsfall orientieren sollte, wobei oft die bereits bekannte Software bevorzugt wird. Im Studium haben wir die Grundlagen der Programmiersprache Python gelernt, wozu es beispielsweise die Library PyMARC zur Verarbeitung von MARC21-Daten. 
 
**Suchmaschinen und Discovery-Systeme (Solr/VuFind)**

In den letzten Veranstaltungen haben wir quasi Daten produziert. Heute ging es darum, diese Daten in einen modernen Bibliothekskatalog zu integrieren und sichtbar zu machen. Dazu haben wir die Open-Source-Software VuFind verwendet, die im Hintergrund mit Apache Solr arbeitet. Solr ist ein Standard für Volltextsuche im Internet und wird unter anderem für die Suche bei Netflix sowie in Bibliothekskatalogen wie Swisscovery eingesetzt. Solr ist also nicht nur auf Bibliotheken beschränkt. Die Installation von VuFind wurde vom Dozenten demonstriert, da der Installationsprozess komplex ist. 

- **Live-Demo des Suchkatalogs auf vufind.org** (Open Source Software): Der Katalog sieht aus wie ein herkömmlicher Bibliothekskatalog und bietet Funktionen wie die Suche nach Relevanz, was typisch ist für Discovery-Systeme. VuFind unterstützt MARC21 und kann bibliografische Daten entsprechend präsentieren. 
- **Installation VuFind 9.0.1 nach offizieller Anleitung unter Ubuntu**: Die Installation erfolgte nach der offiziellen Anleitung, die unter vufind.org verfügbar ist. Der Dozent richtete dafür einen Testserver auf einer virtuellen Maschine unter cloud.digitalocean.com ein, was eine Alternative zu CodesSpace oder einem von der Hochschule bereitgestellten Server darstellt.

  - **Installation**: Der Installationsprozess war sehr technisch und kann im Detail unter diesem Link nachvollzogen werden: https://pad.gwdg.de/HW9D520ORJu79RoIEueNCw#Agenda  
Interessanterweise kann der Solr-Suchindex nur von der Maschine aufgerufen werden, auf der er installiert ist. Um den Zugriff zu ermöglichen, wurde der öffentliche Zugriff auf Solr konfiguriert, wodurch wir später während der Übung auf den Suchindex zugreifen konnten.

  - **Testdaten laden**: Damit wir VuFind ausprobieren konnten, mussten Inhalte in Form von Testdaten in das System geladen werden. VuFind stellt für Testzwecke einige Dateien zur Verfügung. Wir haben davon einige im MARC21-Format hochgeladen und genutzt. 
 
Normalerweise sollte vor dem Import von Daten ein Schema definiert werden, das festlegt, welche Felder existieren und welche Datentypen sie enthalten dürfen.
 
**Solr Schema und Ranking (Exkurs)**

In der Administrationsoberfläche in VuFind gibt es das Schema, in welchem alle von VuFind konfigurierten Felder sind. Dabei gibt es mehrere Titelfelder wie z. B. title, title_full, title_short, um das Relevanz-Ranking zu unterstützen. Wenn ein exakter Titel eingegeben wird, sollte dieser eindeutig an erster Stelle stehen. Dies wird ohne Nutzungsstatistiken, wie sie Google verwendet, erreicht, sondern durch Feldgewichtungen. Wenn jemand den exakten Titel im Feld title_full eingibt, wird dieses Ergebnis am höchsten gewichtet. Wenn der Titel im Untertitel vorkommt, wird er weniger stark gewichtet als im Haupttitel.

Dazu ein Exkurs aus dem 2. Semester im Modul "Information Retrieval". Bei Suchanfragen sind zwei Messgrössen relevant.

- **Recall (Vollständigkeit)**

Der Recall stellt das Verhältnis zwischen den gefundenen relevanten Dokumenten und den vorhandenen relevanten Dokumenten dar.

![Alternativtext](https://jonasbracchi.github.io/bain-lerntagebuch/images/recall.png)

*Recall*

 
- **Precision (Genauigkeit)**

Die Precision ist der Quotient aus der Anzahl der gefundenen relevanten und der Gesamtanzahl der gefundenen Dokumente.

![Alternativtext](https://jonasbracchi.github.io/bain-lerntagebuch/images/precision.png)

*Precision*
 
Dabei ist wichtig zu erwähnen, dass wenn der Recall klein ist, die Precision typischerweise hoch ist (wenige Dokumente in der Ergebnismenge haben eine hohe Precision). Man will dann nur den Informationsbedarf klären und braucht nicht noch andere Dokumente. Und je höher der Recall ist (alle Dokumente in der Datenbank), desto kleiner ist die Precision, weil viele Dokumente aus der Datenbank für mich nicht relevant sind. Man muss sich entsprechend fragen, ob man eine hohe Precision (Informationsbedarf decken) oder einen hohen Recall (Vollständigkeit) haben will.

 
 
 
 
 
 
 
 
 
 

